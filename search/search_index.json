{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to OWA","text":""},{"location":"#open-world-agents-documentation","title":"Open World Agents Documentation","text":"<p>A comprehensive framework for building AI agents that interact with desktop applications through vision, keyboard, and mouse control.</p> <p>Open World Agents (OWA) is a monorepo containing the complete toolkit for multimodal desktop agent development. From high-performance data capture to model training and real-time evaluation, everything is designed for flexibility and performance.</p>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<p>OWA consists of three core components:</p> <p>\ud83c\udf0d Environment (Env) - Asynchronous, event-driven interface for real-time agent interactions \ud83d\udcca Data - High-performance recording, storage, and analysis of multimodal desktop data \ud83e\udd16 Examples - Complete implementations and training pipelines for multimodal agents  </p>"},{"location":"#quick-navigation","title":"Quick Navigation","text":""},{"location":"#environment-framework","title":"\ud83c\udf0d Environment Framework","text":"<p>Build reactive desktop agents with our asynchronous environment system.</p> Component Description Core Concepts <code>Callables</code>, <code>Listeners</code>, and <code>Runnables</code> for real-time processing Environment Guide Dynamic plugin activation and registry patterns Custom Plugins Create your own environment extensions <p>Built-in Plugins: </p> <ul> <li>Desktop Environment - Mouse, keyboard, and window event handling</li> <li>GStreamer Environment - High-performance screen capture (6x faster than alternatives)</li> <li>Standard Environment - Basic utilities and timing functions</li> </ul>"},{"location":"#data-infrastructure","title":"\ud83d\udcca Data Infrastructure","text":"<p>Capture, store, and analyze multimodal desktop interaction data.</p> Component Description Data Overview Complete data pipeline for desktop agents OWAMcap Format Self-contained multimodal data format powered by mcap Desktop Recorder (ocap) High-performance desktop recording tool Data Viewer Visualize and analyze recorded sessions Data Explorer Tools for data exploration and editing"},{"location":"#agent-examples","title":"\ud83e\udd16 Agent Examples","text":"<p>Learn from complete implementations and training pipelines.</p> Example Description Status Multimodal Game Agent Vision-based game playing agent \ud83d\udea7 In Progress GUI Agent General desktop application automation \ud83d\udea7 In Progress Interactive World Model Predictive modeling of desktop environments \ud83d\udea7 In Progress Usage with LLMs Integration with large language models \ud83d\udea7 In Progress Usage with Transformers Vision transformer implementations \ud83d\udea7 In Progress"},{"location":"#community-ecosystem","title":"Community &amp; Ecosystem","text":"<p>\ud83c\udf31 Growing Ecosystem: OWA is designed for extensibility. Community contributions include:  </p> <ul> <li>Custom environment plugins (<code>owa.env.minecraft</code>, <code>owa.env.web</code>, etc.)  </li> <li>Specialized data processors and analyzers  </li> <li>Novel agent architectures and training methods  </li> </ul> <p>\ud83e\udd17 HuggingFace Integration: Upload and share datasets created with <code>ocap</code>. Preview datasets at HuggingFace Spaces.</p>"},{"location":"#development-resources","title":"Development Resources","text":"Resource Description Installation Guide Detailed installation instructions Contributing Guide Development setup, bug reports, feature proposals FAQ Common questions and troubleshooting"},{"location":"#what-can-you-build","title":"What Can You Build?","text":"<p>Anything that runs on desktop. If a human can do it on a computer, you can build an AI agent to automate it:</p> <p>\ud83e\udd16 Desktop Automation - Navigate applications, automate workflows, interact with any software \ud83c\udfae Game AI - Master complex games through visual understanding and real-time decision making \ud83d\udcca Training Datasets - Capture high-quality human-computer interaction data for foundation models \ud83d\udcc8 Benchmarks - Create and evaluate desktop agent performance across diverse tasks</p>"},{"location":"#license","title":"License","text":"<p>This project is released under the MIT License. See the LICENSE file for details.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>We'd love you to contribute to OWA!</p>"},{"location":"contributing/#issues","title":"Issues","text":"<p>Questions, feature requests and bug reports are all welcome as discussions or issues. However, to report a security vulnerability, please see our security policy.</p>"},{"location":"contributing/#pull-requests","title":"Pull Requests","text":"<p>Feel free to create a Pull Request. Project maintainers will take a review quickly and give you a comments.</p> <p>To make contributing as easy and fast as possible, you'll want to run tests and linting locally. Luckily, OWA has few dependencies, doesn't require compiling and tests don't need access to databases, etc. Because of this, setting up and running the tests should be very simple.</p>"},{"location":"contributing/#run-tests","title":"Run tests","text":"<p>We're utilizing <code>pytest</code> for testing and <code>ruff</code> for formatting. Make sure your PR pass all tests in Github Actions.</p> <ol> <li>Run <code>pytest</code>.</li> <li>Run <code>ruff check</code>.</li> <li>Run <code>ruff format</code>.</li> </ol>"},{"location":"contributing/#how-to-test-documentation-changes","title":"How to Test Documentation Changes","text":"<p>If you contribute to the documentation\u2014such as by adding new markdown files under <code>docs/data/</code>\u2014please follow these steps to preview your changes locally before submitting a pull request:</p> <ol> <li> <p>Add your documentation:    Place your new markdown (<code>.md</code>) file in the <code>docs/data/</code> directory.</p> </li> <li> <p>Update navigation:    Edit <code>mkdocs.yaml</code> to include your new file in the site's navigation (<code>nav</code> section).</p> </li> <li> <p>Install documentation dependencies:    At the root of the project, run: <pre><code>vuv install --extra docs\n</code></pre>    This will install all necessary packages for building and serving the documentation site.</p> </li> <li> <p>Serve the documentation locally:    Start the local documentation server with: <pre><code>mkdocs serve\n</code></pre></p> </li> <li> <p>Preview your changes:    Open http://localhost:8000 in your web browser to view the documentation and verify your additions appear as expected.</p> </li> </ol> <p>Following these steps will help ensure your documentation contributions are correctly displayed and formatted. Thank you for helping improve the OWA documentation!</p>"},{"location":"faq_dev/","title":"Faq dev","text":""},{"location":"faq_dev/#how-to-disable-typer-specific-traceback","title":"How to disable <code>typer</code>-specific traceback?","text":"<p>set <code>_TYPER_STANDARD_TRACEBACK=1</code></p> <p>https://stackoverflow.com/questions/76375307/how-to-make-typer-traceback-look-normal</p>"},{"location":"help_with_owa/","title":"Getting help with OWA","text":"<p>If you need help getting started with OWA or with advanced usage, the following sources may be useful.</p>"},{"location":"help_with_owa/#github-discussions","title":"GitHub Discussions","text":"<p>GitHub discussions are useful for asking questions, your question and the answer will help everyone.</p>"},{"location":"help_with_owa/#direct-messages","title":"Direct Messages","text":"<p>If you need further assistance, please feel free to directly message the main contributors via Slack, Discord, or email:</p> <ul> <li>Suhwan Choi: milkclouds00@gmail.com</li> <li>Yunsung Lee: dldbstjd9751@gmail.com</li> </ul>"},{"location":"install/","title":"Installation Guide","text":""},{"location":"install/#quick-start-recommended","title":"Quick Start (Recommended)","text":"<p>For most users who want to use Open World Agents without modifying the source code, installation is straightforward:</p>"},{"location":"install/#option-1-full-installation-with-video-processing","title":"Option 1: Full Installation with Video Processing","text":"<p>If you need desktop recording, screen capture, or video processing capabilities, use conda:</p> <pre><code>conda install owa\n</code></pre> <p>This installs the complete <code>owa</code> meta-package with all dependencies including GStreamer for high-performance video processing.</p>"},{"location":"install/#option-2-headless-installation","title":"Option 2: Headless Installation","text":"<p>For data processing, ML training, or headless servers without video capture needs:</p> <pre><code>pip install owa\n</code></pre> <p>This installs all core functionality except video processing components.</p> <p>When to use conda vs pip</p> <ul> <li> <p>Use <code>conda install owa</code> if you need:</p> <ul> <li>Desktop recording with <code>ocap</code></li> <li>Real-time screen capture</li> <li>Video processing capabilities</li> <li>Complete out-of-the-box experience</li> </ul> </li> <li> <p>Use <code>pip install owa</code> if you:</p> <ul> <li>Only need data processing/analysis</li> <li>Are on a headless server</li> <li>Don't require video capture functionality</li> </ul> </li> </ul>"},{"location":"install/#available-packages","title":"Available Packages","text":"<p>All OWA packages follow lockstep versioning and are available on both PyPI and conda-forge:</p> Name PyPI Conda Description <code>owa</code> Meta-package with all core components <code>owa-core</code> Framework foundation with registry system <code>owa-cli</code> Command-line tools (<code>owl</code>) for data analysis <code>mcap-owa-support</code> OWAMcap format support and utilities <code>ocap</code> \ud83c\udfa5 Desktop recorder for multimodal data capture <code>owa-env-desktop</code> Mouse, keyboard, window event handling <code>owa-env-gst</code> \ud83c\udfa5 GStreamer-powered screen capture (6x faster) <p>\ud83c\udfa5 Video Processing Packages: Packages marked with \ud83c\udfa5 require GStreamer for full functionality. Use <code>conda install</code> for complete features, <code>pip install</code> works for basic functionality.</p>"},{"location":"install/#development-installation-editable","title":"Development Installation (Editable)","text":"<p>For Contributors and Developers</p> <p>This section is for users who want to modify the source code, contribute to the project, or need the latest development features.</p>"},{"location":"install/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding with development installation, ensure you have the necessary tools:</p> <ol> <li>Git: For cloning the repository</li> <li>Python 3.11+: Required for all OWA packages</li> <li>Virtual Environment Tool: We recommend conda/mamba for complete functionality</li> </ol>"},{"location":"install/#step-1-setup-virtual-environment","title":"Step 1: Setup Virtual Environment","text":"conda/mamba (Recommended)Other Virtual Environments <ol> <li> <p>Install miniforge following the installation guide:     <pre><code># Download and install miniforge\n# This provides both conda and mamba (faster conda)\n</code></pre></p> </li> <li> <p>Create and activate your environment:     <pre><code>conda create -n owa-dev python=3.11 -y\nconda activate owa-dev\n</code></pre></p> </li> <li> <p>(Required for video processing) Install GStreamer dependencies:     <pre><code># Clone the repo first to access environment.yml\ngit clone https://github.com/open-world-agents/open-world-agents\ncd open-world-agents\n\n# Install GStreamer and related dependencies\nmamba env update --name owa-dev --file projects/owa-env-gst/environment.yml\n</code></pre></p> </li> </ol> <p>You can use other virtual environment tools (venv, virtualenv, poetry, etc.), but:</p> <ul> <li>GStreamer must be installed separately for video processing functionality</li> <li>Installation complexity increases due to GStreamer's native dependencies</li> <li>We recommend conda/mamba for the best development experience</li> </ul>"},{"location":"install/#step-2-clone-and-setup-development-tools","title":"Step 2: Clone and Setup Development Tools","text":"<pre><code># Clone the repository\ngit clone https://github.com/open-world-agents/open-world-agents\ncd open-world-agents\n\n# Install uv (fast Python package manager)\npip install uv\n\n# Install virtual-uv for easier monorepo management\npip install virtual-uv\n</code></pre>"},{"location":"install/#step-3-install-in-editable-mode","title":"Step 3: Install in Editable Mode","text":"uv + virtual-uv (Recommended)uv (Simple)pip (Manual) <pre><code># Ensure you're in the project root and environment is activated\ncd open-world-agents\nconda activate owa-dev  # or your environment name\n\n# Install all packages in editable mode\nvuv install\n</code></pre> <p>Tip</p> <p><code>vuv</code> (virtual-uv) handles the complex dependency resolution for our monorepo structure and installs all packages in the correct order.</p> <pre><code># Install with inexact dependency resolution\nuv sync --inexact\n</code></pre> <pre><code># Install in correct order (dependency order matters with pip)\npip install -e projects/owa-core\npip install -e projects/mcap-owa-support\npip install -e projects/owa-env-desktop\npip install -e projects/owa-env-gst  # Requires GStreamer\npip install -e projects/owa-cli\npip install -e projects/ocap\n</code></pre> <p>Installation Order Matters</p> <p>When using <code>pip</code> instead of <code>uv</code>, the installation order is critical because <code>pip</code> cannot resolve the monorepo dependencies specified in <code>[tool.uv.sources]</code>.</p>"},{"location":"install/#step-4-verify-installation","title":"Step 4: Verify Installation","text":"<pre><code># Test core functionality\npython -c \"from owa.core.registry import CALLABLES; print('\u2705 Core installed')\"\n\n# Test CLI tools\nowl --help\nocap --help\n\n# Test GStreamer install if you need it\npython -c \"import gi; gi.require_version('Gst', '1.0'); print('\u2705 GStreamer OK')\"\n</code></pre>"},{"location":"install/#troubleshooting","title":"Troubleshooting","text":""},{"location":"install/#gstreamer-issues","title":"GStreamer Issues","text":"<p>If you encounter GStreamer-related errors:</p> <ol> <li>Ensure conda environment: GStreamer installation works best through conda</li> <li>Update environment file:    <pre><code>mamba env update --name your-env --file projects/owa-env-gst/environment.yml\n</code></pre></li> <li>Check GStreamer installation:    <pre><code>python -c \"import gi; gi.require_version('Gst', '1.0'); print('\u2705 GStreamer OK')\"\n</code></pre></li> </ol>"},{"location":"install/#virtual-environment-issues","title":"Virtual Environment Issues","text":"<ul> <li>Always activate your environment before running <code>vuv</code> or installation commands</li> <li>Use absolute paths if you encounter import issues</li> <li>Reinstall virtual-uv if you encounter dependency resolution problems:   <pre><code>pip uninstall virtual-uv\npip install virtual-uv\n</code></pre></li> </ul>"},{"location":"install/#package-version-conflicts","title":"Package Version Conflicts","text":"<p>OWA uses lockstep versioning. If you encounter version conflicts:</p> <pre><code># Check installed versions\npip list | grep owa\n\n# Reinstall with matching versions\npip install owa-core==0.3.2 owa-cli==0.3.2 owa-env-desktop==0.3.2\n</code></pre>"},{"location":"data/","title":"Open-sourcing Dataset for Multimodal Desktop Agent","text":"<p>As of now (March 22, 2025), there are few datasets available for building multimodal desktop agents.</p> <p>Even more scarce are datasets that (1) contain high-frequency screen data, (2) have keyboard/mouse information timestamp-aligned with other modalities like screen recordings, and (3) include human demonstrations.</p> <p>To address this gap, open-world-agents provides the following three solutions:</p> <ol> <li> <p>File Format - <code>OWAMcap</code>: A high-performance, self-contained, flexible container file format for multimodal desktop log data, powered by the open-source container file format mcap. Learn more...</p> </li> <li> <p>Desktop Recorder - <code>ocap your-filename.mcap</code>: A powerful, efficient, and easy-to-use desktop recorder that captures keyboard/mouse and high-frequency screen data.</p> <ul> <li>Powered by <code>owa-env-gst</code>, ensuring superior performance compared to alternatives. Learn more...</li> </ul> </li> <li> <p>\ud83e\udd17 Hugging Face Integration &amp; Community Ecosystem: The largest collection of open-source desktop interaction datasets in OWAMcap format.</p> <ul> <li>Growing Dataset Collection: Hundreds of community-contributed datasets covering diverse workflows, applications, and interaction patterns</li> <li>Easy Upload &amp; Sharing: Upload your <code>ocap</code> recordings directly to HuggingFace with one command</li> <li>Standardized Format: All datasets use the unified OWAMcap format for seamless integration</li> <li>Interactive Visualization: Preview any dataset at Hugging Face Spaces</li> <li>Browse Available Datasets: \ud83e\udd17 datasets?other=owamcap</li> </ul> </li> </ol> <p>\ud83d\ude80 Community Impact: With OWA's streamlined recording and sharing pipeline, the open-source desktop agent community has rapidly grown from zero to hundreds of publicly available multimodal datasets, democratizing access to high-quality training data.</p>"},{"location":"data/comparison_with_lerobot/","title":"OWAMcap vs LeRobotDataset: A Technical Comparison","text":""},{"location":"data/comparison_with_lerobot/#executive-summary","title":"Executive Summary","text":"<p>Both OWAMcap and LeRobotDataset address the critical need for standardized multimodal data formats in embodied AI. However, they differ significantly in their architectural approach and target domains. This comparison analyzes three distinct layers: container format, data schema, and library ecosystem.</p>"},{"location":"data/comparison_with_lerobot/#three-layer-comparison-framework","title":"Three-Layer Comparison Framework","text":"<p>To properly compare OWAMcap and LeRobotDataset, we need to understand that they operate at different architectural levels. Rather than comparing them directly, we analyze three distinct layers of the data stack:</p> <p>Why Three Layers Matter:</p> <ul> <li>Container Format: Think of this as your storage unit\u2014how you pack your stuff (MCAP vs Parquet)</li> <li>Data Schema: This is what you actually put in those boxes\u2014the \"language\" your data speaks (OWAMcap vs LeRobotDataset)</li> <li>Library Ecosystem: The tools and trucks you need to move everything around (mcap-owa-support vs lerobot)</li> </ul> <p>This separation matters because without it, we'd be comparing fundamentally different things. It's like trying to compare a car's engine (container format) with its GPS system (data schema) with its maintenance costs (library ecosystem)\u2014they're all important, but they solve different problems and need to be evaluated on their own terms.</p>"},{"location":"data/comparison_with_lerobot/#layer-1-container-format-mcap-vs-parquet","title":"Layer 1: Container Format (MCAP vs Parquet)","text":"<p>Imagine you're organizing your digital life. MCAP is like having a smart filing cabinet that automatically timestamps everything and keeps related items together. Parquet? That's more like Excel on steroids\u2014fantastic for crunching numbers, but ask it to handle your mixed media collection and things get messy.</p> Feature MCAP Parquet (LeRobotDataset) Primary Design Time-synchronized multimodal logging Columnar analytics storage Data Organization Multiple channels/topics with explicit schemas Single table structure Heterogeneous Data \u2705 Native support for mixed data types \u274c Tabular data only; external file references Time Synchronization \u2705 Per-message timestamps with indexing \u274c Manual alignment across files required Streaming Safety \u2705 Crash-safe incremental writes \u274c Bulk writes; vulnerable to data loss Random Access \u2705 Indexed time/channel queries \u274c Sequential column scans Schema Extensibility \u2705 Custom message types supported \u274c Fixed table schema Self-Containedness \u2705 Embedded schemas and metadata \u274c External dependencies for interpretation"},{"location":"data/comparison_with_lerobot/#layer-2-data-format-owamcap-vs-lerobotdataset","title":"Layer 2: Data Format (OWAMcap vs LeRobotDataset)","text":"<p>While MCAP vs Parquet represents the container comparison, OWAMcap vs LeRobotDataset represents the data schema comparison\u2014how domain-specific message types and structures are defined on top of these containers.</p> <p>Commonalities: Both use lazy-loading for video frames to optimize storage and memory usage.</p> <p>Key Differences:</p> <pre><code># OWAMcap: Desktop-specific message types\nclass ScreenEmitted(OWAMessage):\n    path: str           # Video file reference\n    pts: int           # Precise frame timestamp\n    utc_ns: int        # System timestamp\n\nclass MouseEvent(OWAMessage):\n    event_type: str    # move, click, scroll\n    x: int, y: int     # Screen coordinates\n\nclass KeyboardEvent(OWAMessage):\n    event_type: str    # press, release\n    vk: int           # Virtual key code\n</code></pre> <pre><code># LeRobotDataset: Generic robotics observations\n{\n    \"observation.image\": \"path/to/frame.jpg\",\n    \"observation.state\": [x, y, z, ...],  # Robot joint positions\n    \"action\": [dx, dy, dz, ...]           # Action commands\n}\n</code></pre> <p>Domain Specialization Impact:</p> <ul> <li>OWAMcap: Pre-defined messages enables seamless integration across diverse desktop tasks (web browsing, document editing, gaming)</li> <li>LeRobotDataset: Generic structure requires domain-specific adaptations for each robot platform</li> </ul>"},{"location":"data/comparison_with_lerobot/#layer-3-library-ecosystem","title":"Layer 3: Library Ecosystem","text":"<p>Library Design Philosophy:</p> <p>The fundamental difference reflects two approaches: minimal dependencies (OWAMcap) for worry-free adoption vs comprehensive ecosystem (LeRobotDataset) bundling complete toolchains.</p> Metric mcap-owa-support lerobot Dependencies 21 packages 93 packages Install Time 0.75s 66.65s Adoption Friction \"Just works\" territory \"Hope nothing breaks\" zone <p>Dependency Analysis:</p> <pre><code># OWAMcap: The minimalist's dream\nmcap-owa-support\n\u251c\u2500\u2500 mcap (the core engine)\n\u251c\u2500\u2500 pydantic (keeps data honest)\n\u251c\u2500\u2500 loguru (friendly logging)\n\u2514\u2500\u2500 zstandard (compression magic)\n\n# LeRobotDataset: The everything ecosystem\nlerobot\n\u251c\u2500\u2500 torch + torchvision (GPU go brrrr)\n\u251c\u2500\u2500 gym + mujoco (virtual robot playground)\n\u251c\u2500\u2500 opencv + imageio (pixel manipulation station)\n\u251c\u2500\u2500 wandb (experiment diary)\n\u251c\u2500\u2500 hydra (configuration wizard)\n\u2514\u2500\u2500 [85+ more packages having a dependency party]\n</code></pre> <p>The Zero-Friction Philosophy \ud83d\udca1</p> <p>Our guiding principle is simple: developers should install our library and immediately get back to building cool stuff, not debugging dependency conflicts or waiting for installations to finish.</p>"},{"location":"data/comparison_with_lerobot/#why-container-choice-matters-for-foundation-models","title":"Why Container Choice Matters for Foundation Models","text":""},{"location":"data/comparison_with_lerobot/#random-access-the-need-for-speed","title":"Random Access: The Need for Speed","text":"<p>The difference between MCAP and Parquet for data access is like comparing a sports car to a city bus. Both get you there, but the experience is... different.</p> <pre><code># MCAP: \"I want data from 2:30 PM to 2:35 PM, please\"\nmessages = reader.iter_messages(\n    start_time=start_ns,\n    end_time=end_ns,\n    topics=[\"screen\", \"mouse\"]\n)  # Boom. Done. Lightning fast.\n\n# Parquet: \"Let me read everything and then filter...\"\ndf = pd.read_parquet(\"data.parquet\")\nfiltered = df[(df.timestamp &gt;= start) &amp; (df.timestamp &lt;= end)]\n# *waiting music intensifies*\n</code></pre>"},{"location":"data/comparison_with_lerobot/#multi-modal-synchronization-keeping-everyone-in-sync","title":"Multi-Modal Synchronization: Keeping Everyone in Sync","text":"<p>MCAP: Like a conductor with perfect timing\u2014every instrument (modality) hits their notes exactly when they should.</p> <pre><code>Channel 1: screen     [t1, t3, t5, t7, ...]\nChannel 2: mouse      [t1, t2, t4, t6, t8, ...]\nChannel 3: keyboard   [t2, t5, t9, ...]\n</code></pre> <p>Parquet: More like a garage band where everyone's trying to stay in time but someone's always slightly off-beat.</p>"},{"location":"data/comparison_with_lerobot/#desktop-vs-robotics-two-different-worlds","title":"Desktop vs Robotics: Two Different Worlds","text":"Domain Desktop Automation Robotics Session Length Hours of continuous interaction Minutes of task execution Event Frequency High-frequency input events Lower-frequency control commands Crash Recovery Critical for long sessions Less critical for short episodes Data Types Window focus, UI interactions, multi-monitor Joint positions, sensor readings, control commands"},{"location":"data/comparison_with_lerobot/#performance-implications-for-vla-training","title":"Performance Implications for VLA Training","text":""},{"location":"data/comparison_with_lerobot/#storage-efficiency","title":"Storage Efficiency","text":"<pre><code># Example 45-min desktop session\nMetadata (mcap):     24 MiB\nVideo (external):    5.4 GiB\nTotal:              5.4 GiB\n\n# Equivalent data in uncompressed format\nRaw frames:         ~447 GiB\nCompression ratio:  82x reduction\n</code></pre>"},{"location":"data/comparison_with_lerobot/#training-pipeline-impact","title":"Training Pipeline Impact","text":"<p>\ud83d\udea7 TODO: Here is TODO and subject to be changed.</p> <p>Data Loading Performance: <pre><code># OWAMcap: Efficient batch loading with precise temporal control\nfor batch in dataloader:\n    # Direct access to synchronized multimodal streams\n    screens = [msg.lazy_load() for msg in batch.screen_messages]\n    actions = batch.mouse_events + batch.keyboard_events\n    # No resampling artifacts; preserves original event timing\n\n# LeRobotDataset: The \"close enough\" approach\nfor batch in dataloader:\n    # delta_timestamps is the key design\n    frames = dataset[i:i+batch_size]\n    # Manual synchronization across heterogeneous streams required\n</code></pre></p> <p>Write Performance:</p> Scenario MCAP (OWAMcap) Parquet (LeRobotDataset) Real-time logging \u2705 Optimized append-only writes \u274c Requires batching; write overhead High-frequency events \u2705 Native support \u274c Must aggregate before writing Crash recovery \u2705 Partial file recovery possible \u274c Risk of data loss during writes"},{"location":"data/comparison_with_lerobot/#schema-evolution-and-fair-data-principles","title":"Schema Evolution and FAIR Data Principles","text":"<p>Schema Evolution:</p> <ul> <li>OWAMcap: Each channel maintains independent schema; new modalities added without affecting existing data</li> <li>LeRobotDataset: Global schema changes affect entire dataset</li> </ul> <p>FAIR Data Alignment:</p> Principle OWAMcap LeRobotDataset Findable \u2705 Rich embedded metadata \u26a0\ufe0f Depends on HF Hub infrastructure Accessible \u2705 Self-contained files \u26a0\ufe0f Multi-file dependencies Interoperable \u2705 Standard MCAP readers \u2705 HF ecosystem compatibility Reusable \u2705 Embedded schemas + provenance \u26a0\ufe0f External documentation required"},{"location":"data/comparison_with_lerobot/#strategic-recommendations","title":"Strategic Recommendations","text":""},{"location":"data/comparison_with_lerobot/#the-decision-matrix","title":"The Decision Matrix","text":"Use Case Recommended Format Why This Makes Sense Desktop Foundation Models OWAMcap Purpose-built, lightweight, just works Production Desktop Agents OWAMcap Zero dependencies headaches, crash-safe Novel Multimodal Research OWAMcap Flexibility to experiment without limits Academic Robotics Research LeRobotDataset Join the party everyone's already at"},{"location":"data/comparison_with_lerobot/#the-hybrid-approach-best-of-both-worlds","title":"The Hybrid Approach: Best of Both Worlds","text":"<p>For the ambitious researchers who want it all:</p> <ol> <li>Capture Phase: Use OWAMcap to grab everything (think of it as your digital net)</li> <li>Consumption Phase: Transform relevant bits for your ML pipeline (curated data delivery)</li> </ol>"},{"location":"data/comparison_with_lerobot/#conclusion-the-plot-twist-ending","title":"Conclusion: The Plot Twist Ending","text":"<p>Here's the thing\u2014OWAMcap and LeRobotDataset aren't really competitors. They're more like specialized tools designed for different jobs. OWAMcap is the precision instrument for desktop automation\u2014lightweight, focused, and built for the unique chaos of human-computer interaction. LeRobotDataset(rather, LeRobot) is the comprehensive toolkit for robotics research\u2014heavy-duty, feature-rich, and backed by a thriving community.</p> <p>The real question isn't \"which is better?\" but \"which fits your mission?\" If you're building the next generation of desktop AI agents, OWAMcap's specialized design will save you months of headaches. If you're advancing robotics research within existing academic frameworks, LeRobot's ecosystem might be your golden ticket.</p> <p>The future of embodied AI isn't about choosing sides\u2014it's about picking the right tool for the job and maybe, just maybe, building bridges between these different worlds. After all, the best AI systems might need to understand both digital desktops and physical robots. Now wouldn't that be something? \ud83d\ude80</p>"},{"location":"data/data_format/","title":"Redirected","text":"<p>This page has moved to data/infographic.html.</p> <p>You will be automatically redirected in a few seconds.</p> <p></p>"},{"location":"data/data_format_v1/","title":"Introducing OWAMcap","text":""},{"location":"data/data_format_v1/#overview","title":"Overview","text":"<p>OWAMcap is a high-performance, self-contained, flexible container file format for multimodal desktop log data, powered by the open-source container file format mcap. This format is designed for efficiently recording and processing message data in Open World Agents (OWA) applications.</p> <p>So, what exactly is mcap?</p> <p>Simply put, mcap is a format that allows you to record various types of events such as keyboard events, mouse events, and screen captures along with their timestamps. For more detailed information, please refer to the OWAMcap Format Specification section.</p>"},{"location":"data/data_format_v1/#usage-example-of-owamcap-desktop-recorder","title":"Usage Example of OWAMcap - Desktop Recorder","text":"<p>What exactly does the OWAMcap format contain? Let's demonstrate with an example of recorded desktop data. Below are sample datasets that you can download and explore yourself:</p> <ul> <li><code>example.mcap</code> [Download]</li> <li><code>example.mkv</code> [Download]</li> </ul> Click here to see <code>example.mkv</code>! <p> </p>"},{"location":"data/data_format_v1/#exploring-example-data","title":"Exploring Example Data","text":"<p>Let's examine the contents of an OWAMcap file using the <code>owl</code> command-line tool (Open World agents cLi).</p>"},{"location":"data/data_format_v1/#file-summary-with-owl-mcap-info","title":"File Summary with <code>owl mcap info</code>","text":"<p>First, we can get an overview of the file structure:</p> <pre><code>$ owl mcap info example.mcap\nlibrary:   mcap-owa-support 0.1.0; mcap 1.2.2\nprofile:   owa\nmessages:  518\nduration:  6.8558623s\nstart:     2025-03-21T17:06:30.7029335+09:00 (1742544390.702933500)\nend:       2025-03-21T17:06:37.5587958+09:00 (1742544397.558795800)\ncompression:\n        zstd: [1/1 chunks] [48.19 KiB/9.42 KiB (80.44%)] [1.37 KiB/sec]\nchannels:\n        (1) window            7 msgs (1.02 Hz)    : owa.env.desktop.msg.WindowInfo [jsonschema]\n        (2) keyboard/state    7 msgs (1.02 Hz)    : owa.env.desktop.msg.KeyboardState [jsonschema]\n        (3) mouse/state       7 msgs (1.02 Hz)    : owa.env.desktop.msg.MouseState [jsonschema]\n        (4) mouse           115 msgs (16.77 Hz)   : owa.env.desktop.msg.MouseEvent [jsonschema]\n        (5) screen          362 msgs (52.80 Hz)   : owa.env.gst.msg.ScreenEmitted [jsonschema]\n        (6) keyboard         20 msgs (2.92 Hz)    : owa.env.desktop.msg.KeyboardEvent [jsonschema]\nchannels: 6\nattachments: 0\nmetadata: 0\n</code></pre> <p>Key observations from this output:</p> <ol> <li> <p>File Overview:</p> <ul> <li>Contains 518 messages recorded over 6.86 seconds</li> <li>Records from March 21, 2025, with precise start and end timestamps</li> </ul> </li> <li> <p>Compression:</p> <ul> <li>Uses zstd compression, reducing file size by 80.44%</li> </ul> </li> <li> <p>Channels (Topics):</p> <ul> <li>The file contains 6 different channels (or topics), each tracking a specific type of event:</li> </ul> </li> </ol> Channel # Name Message Count Frequency Message Type 1 window 7 msgs 1.02 Hz WindowInfo 2 keyboard/state 7 msgs 1.02 Hz KeyboardState 3 mouse/state 7 msgs 1.02 Hz MouseState 4 mouse 115 msgs 16.77 Hz MouseEvent 5 screen 362 msgs 52.80 Hz ScreenEmitted 6 keyboard 20 msgs 2.92 Hz KeyboardEvent <p>For example, looking at channel #5 (screen), we can see:</p> <ul> <li>The topic name is \"screen\"</li> <li>It contains 362 messages</li> <li>Recording frequency is 52.80 Hz (slightly lower than the intended 60 Hz, likely due to the short recording time)</li> <li>Messages are of type <code>owa.env.gst.msg.ScreenEmitted</code></li> </ul>"},{"location":"data/data_format_v1/#detailed-message-inspection-with-owl-mcap-cat","title":"Detailed Message Inspection with <code>owl mcap cat</code>","text":"<p>To examine individual messages, we can use the <code>cat</code> command:</p> <pre><code>$ owl mcap cat example.mcap --n 8 --no-pretty\nTopic: window, Timestamp: 1741628814049712700, Message: {'title': 'ZType \u2013 Typing Game - Type to Shoot - Chromium', 'rect': [389, 10, 955, 1022], 'hWnd': 7540094}\nTopic: keyboard/state, Timestamp: 1741628814049712700, Message: {'buttons': []}\nTopic: mouse/state, Timestamp: 1742544390703436600, Message: {'x': 1594, 'y': 1112, 'buttons': []}\nTopic: mouse, Timestamp: 1742544390707441200, Message: {'event_type': 'move', 'x': 1597, 'y': 1112}\nTopic: screen, Timestamp: 1741628814057575300, Message: {'path': 'example.mkv', 'pts': 14866666666, 'utc_ns': 1741628814056571100}\nTopic: screen, Timestamp: 1741628814073392700, Message: {'path': 'example.mkv', 'pts': 14883333333, 'utc_ns': 1741628814072476900}\nTopic: keyboard, Timestamp: 1741628815015522100, Message: {'event_type': 'release', 'vk': 162}\n</code></pre> <p>What we can learn from these messages:</p> <ol> <li> <p>Window messages - Track active windows</p> <ul> <li>Example: <code>{'title': 'ZType \u2013 Typing Game - Type to Shoot - Chromium', 'rect': [389, 10, 955, 1022], 'hWnd': 7540094}</code></li> <li>Shows which window was active, its title, position and size</li> </ul> </li> <li> <p>Mouse messages - Track cursor position and button states</p> <ul> <li>Position tracking: <code>{'x': 1597, 'y': 1112}</code></li> <li>Event types include: \"move\", \"click\", etc.</li> </ul> </li> <li> <p>Keyboard messages - Track key presses and releases</p> <ul> <li>Example: <code>{'event_type': 'release', 'vk': 162}</code></li> <li>Records which virtual key was pressed or released</li> </ul> </li> <li> <p>Screen messages - Link to video frames in the MKV file</p> <ul> <li>Contains paths, presentation timestamps, and UTC timestamps</li> </ul> </li> </ol>"},{"location":"data/data_format_v1/#using-this-data","title":"Using This Data","text":"<p>This structured data allows for powerful analysis and use cases:</p> <ul> <li>You can filter data based on which window was active at a particular time</li> <li>You can synchronize keyboard/mouse events with screen captures</li> <li>The timestamps allow for precise reconstruction of user interactions</li> </ul> <p>What's VK(Virtual Key Code)?</p> <p>Operating systems don't directly use the physical keyboard input values (scan codes) but instead use virtualized keys called VKs. OWA's recorder uses VKs to record keyboard-agnostic data. If you're interested in more details, you can refer to the following resources:</p> <ul> <li>Keyboard Input Overview, Microsoft</li> <li>Virtual-Key Codes, Microsoft</li> </ul>"},{"location":"data/data_format_v1/#internals-owamcap-format-specification","title":"Internals - OWAMcap Format Specification","text":"<p>Note for Users</p> <p>This part is intended for developers who want to utilize the OWAMcap file format for their own applications. Regular users of the library may not need this information.</p>"},{"location":"data/data_format_v1/#technical-specifications","title":"Technical Specifications","text":"<ul> <li>OWAMcap uses the standard <code>mcap</code> format with <code>json</code> schema</li> <li>The <code>mcap-owa-support</code> Python package, which is within the open-world-agents repository, provides decoders, writers, and readers for this format</li> <li>All messages must inherit from or implement the <code>BaseMessage</code> class from <code>owa.core.message</code></li> </ul> <p>What's MCAP?</p> <p>MCAP (pronounced \"em-cap\") is an open-source container file format designed for multimodal log data. It supports multiple channels of timestamped pre-serialized data and is ideal for pub/sub or robotics applications.</p> <p>Key advantages of MCAP:</p> <ul> <li>High Performance: Efficient storage and retrieval of large event data streams</li> <li>Flexible &amp; Open: Works with diverse data types beyond robotics</li> <li>Self-Describing: Encodes schema information to ensure compatibility</li> </ul> <p>Learn more about MCAP</p>"},{"location":"data/data_format_v1/#implementation-guide","title":"Implementation Guide","text":"<p>Any message that implements <code>BaseMessage</code> can be recorded in the OWAMcap format. This provides flexibility while maintaining a consistent interface. Following block describes the interface of <code>BaseMessage</code>.</p> <pre><code>class BaseMessage(ABC):\n    _type: str\n\n    @abstractmethod\n    def serialize(self, buffer: io.BytesIO): ...\n\n    @classmethod\n    @abstractmethod\n    def deserialize(cls, buffer: io.BytesIO) -&gt; Self: ...\n\n    @classmethod\n    @abstractmethod\n    def get_schema(cls): ...\n</code></pre>"},{"location":"data/data_format_v1/#file-format-considerations","title":"File Format Considerations","text":""},{"location":"data/data_format_v1/#why-use-mcap","title":"Why Use <code>.mcap</code>?","text":"<p>There are very few open-source formats available for heterogeneous timestamped data. ROS's bagfile format is one option, but it heavily depends on the ROS ecosystem and often requires installation of ROS1/2. In comparison, <code>mcap</code> is self-contained and efficient, especially for random read (or seeking) operations, which is critical for training VLA (Vision-Language-Action) models.</p>"},{"location":"data/data_format_v2/","title":"Introducing OWAMcap","text":""},{"location":"data/data_format_v2/#overview","title":"Overview","text":"<p>OWAMcap is a specification for using the open-source mcap container file format with Open World Agents (OWA) message definitions. It defines how to structure multimodal desktop log data within standard mcap files using OWA-specific message schemas.</p> <p>What makes a file \"OWAMcap\":</p> <ul> <li>Standard mcap file format with OWA profile designation</li> <li>OWA's predefined message types for desktop interaction data (mouse, keyboard, screen, etc.)</li> <li>Optimized storage strategies (e.g., external video files referenced from mcap)</li> </ul> <p>So, what exactly is mcap?</p> <p>mcap is a format that records various timestamped events like keyboard inputs, mouse movements, and screen captures. OWAMcap leverages this by defining specific message schemas for desktop interaction data. See the Format Specification for details.</p> <p> Visualization of how OWAMcap stores multimodal data with precise timestamps</p>"},{"location":"data/data_format_v2/#the-vision-a-universal-standard-for-desktop-interaction-data","title":"The Vision: A Universal Standard for Desktop Interaction Data","text":""},{"location":"data/data_format_v2/#the-current-problem","title":"The Current Problem","text":"<p>The biggest obstacle to building foundation models for desktop automation is data fragmentation. Each research group collects data in proprietary formats, making it nearly impossible to combine datasets. This mirrors the robotics community, where enormous resources are continuously wasted converting between incompatible formats instead of advancing research.</p> <p>Real-World Example: Open-X Embodiment's Herculean Effort</p> <p>The Open-X Embodiment project perfectly illustrates this challenge. To create their unified robotics dataset, researchers had to:</p> <ul> <li>Manually convert 22 different datasets from completely different formats</li> <li>Spend months writing custom parsers for each proprietary format</li> <li>Standardize action spaces, observation formats, and metadata schemas</li> <li>Validate data integrity across heterogeneous sources</li> <li>Maintain conversion scripts as source datasets evolved</li> </ul> <p>This massive undertaking required an entire team's effort for what should have been a straightforward data combination task. The same challenge now faces desktop automation.</p> <p> Illustration of the exponential complexity when converting between N different proprietary formats</p>"},{"location":"data/data_format_v2/#owamcap-as-the-universal-standard","title":"OWAMcap as the Universal Standard","text":"<p>OWAMcap establishes a unified foundation that enables:</p> <p>\ud83c\udfaf Seamless Data Integration - Datasets from different organizations can be directly combined - No costly conversion processes between proprietary formats - Enables building truly large-scale, diverse training datasets</p> <p>\ud83d\ude80 Foundation Model Enablement - Aggregated data from multiple sources in a unified format - Efficient random access for training large models - Standardized preprocessing pipelines across the community</p> <p>\ud83d\udd17 Breaking Down Data Silos Imagine a future where: - Research institutions directly share desktop interaction datasets - Companies contribute to common training pools without format barriers - Individual researchers access and combine datasets from multiple sources seamlessly - Foundation models train on massive, diverse datasets spanning different applications</p> <p> Before: Fragmented data silos requiring costly conversion. After: Direct dataset combination with OWAMcap</p>"},{"location":"data/data_format_v2/#the-community-impact","title":"The Community Impact","text":"<p>By establishing OWAMcap as a standard, we redirect enormous resources currently spent on data format conversion toward actual research and model development. This is particularly crucial for foundation models, which require vast amounts of diverse data to achieve their full potential.</p> <p>Preventing the 'Format Wars'</p> <p>The desktop automation field is at a critical juncture. Without standardization, we risk repeating robotics' mistakes: researchers spending months on format conversion, valuable datasets remaining isolated, and foundation models unable to reach their potential due to fragmented training data.</p>"},{"location":"data/data_format_v2/#technical-innovation-hybrid-storage-strategy","title":"Technical Innovation: Hybrid Storage Strategy","text":"<p> OWAMcap's innovative approach: lightweight mcap metadata with external video storage</p> <p>OWAMcap's most innovative feature is its approach to video data:</p> <ul> <li>Video data: Stored in external files (<code>.mkv</code>) with efficient encoding</li> <li>Metadata: Stored in mcap with precise timestamps and frame references</li> <li>Result: Minimal file sizes with frame-accurate synchronization</li> </ul> <pre><code>class ScreenEmitted(OWAMessage):\n    _type = \"owa.env.gst.msg.ScreenEmitted\"\n\n    # Timestamps and frame references\n    utc_ns: int | None = None\n    path: str | None = None  # e.g., \"example.mkv\"\n    pts: int | None = None   # Precise frame timestamp\n\n    # Optional in-memory frame data\n    frame_arr: Optional[np.ndarray] = Field(None, exclude=True)\n    shape: Optional[Tuple[int, int]] = None\n\n    def lazy_load(self) -&gt; np.ndarray:\n        \"\"\"Load frame data on-demand from external video file.\"\"\"\n        if self.frame_arr is None and self.path and self.pts:\n            rgb_array = _video_reader.get_frame_at_pts(self.path, self.pts)\n            self.frame_arr = cv2.cvtColor(rgb_array, cv2.COLOR_RGB2BGRA)\n        return self.frame_arr\n</code></pre> <p>Benefits: Storage efficiency, library compatibility, lazy loading, and seamless integration with existing video tools.</p> <p> Storage size comparison: Traditional formats vs. OWAMcap's hybrid approach</p>"},{"location":"data/data_format_v2/#usage-example","title":"Usage Example","text":"<p>Sample datasets demonstrating the format:</p> <ul> <li><code>example.mcap</code> [Download] - Metadata and timestamps</li> <li><code>example.mkv</code> [Download] - Video data</li> </ul>    Your browser does not support the video tag.  <p>Interactive demonstration of loading and exploring OWAMcap data</p>"},{"location":"data/data_format_v2/#file-overview","title":"File Overview","text":"<pre><code>$ owl mcap info example.mcap\nlibrary:   mcap-owa-support 0.3.2; mcap 1.2.2\nprofile:   owa\nmessages:  751029\nduration:  1h27m38.9810357s\ncompression: zstd (86.96% reduction)\nchannels:\n    (1) window          5257 msgs (1.00 Hz) : WindowInfo\n    (2) keyboard/state  5256 msgs (1.00 Hz) : KeyboardState  \n    (3) mouse/state     5256 msgs (1.00 Hz) : MouseState\n    (4) screen        303524 msgs (57.72 Hz): ScreenEmitted\n    (5) mouse         429580 msgs (81.69 Hz): MouseEvent\n    (6) keyboard        2156 msgs (2.92 Hz) : KeyboardEvent\n</code></pre> <p>Key insight: Only 21 MiB for 1.5 hours of multimodal data, thanks to external video storage.</p>"},{"location":"data/data_format_v2/#message-examples","title":"Message Examples","text":"<pre><code>$ owl mcap cat example.mcap --n 4 --no-pretty\nTopic: window, Message: {'title': 'ZType \u2013 Typing Game - Chromium', 'rect': [389, 10, 955, 1022]}\nTopic: mouse, Message: {'event_type': 'move', 'x': 1597, 'y': 1112}\nTopic: screen, Message: {'path': 'example.mkv', 'pts': 14866666666, 'utc_ns': 1741628814056571100}\nTopic: keyboard, Message: {'event_type': 'release', 'vk': 162}\n</code></pre> <p>This structured data enables precise reconstruction of user interactions synchronized with screen captures, and most importantly, direct combination with datasets from other sources.</p>"},{"location":"data/data_format_v2/#format-specification","title":"Format Specification","text":""},{"location":"data/data_format_v2/#technical-definition","title":"Technical Definition","text":"<p>OWAMcap consists of:</p> <ul> <li>Base: Standard mcap format with JSON schema</li> <li>Profile: <code>owa</code> designation in mcap metadata  </li> <li>Messages: Must implement <code>BaseMessage</code> interface</li> <li>Support: <code>mcap-owa-support</code> package for reading/writing</li> </ul> <pre><code>class BaseMessage(ABC):\n    _type: str\n\n    @abstractmethod\n    def serialize(self, buffer: io.BytesIO): ...\n\n    @classmethod  \n    @abstractmethod\n    def deserialize(cls, buffer: io.BytesIO) -&gt; Self: ...\n\n    @classmethod\n    @abstractmethod\n    def get_schema(cls): ...\n</code></pre>"},{"location":"data/data_format_v2/#design-rationale","title":"Design Rationale","text":"<p>Why mcap? </p> <p>Few open-source formats support heterogeneous timestamped data. ROS bagfiles require heavy ROS dependencies, while mcap is self-contained and optimized for random access\u2014critical for VLA model training.</p> <p>Why external video storage?</p> <ul> <li>Video codecs (H.264, H.265) are highly optimized</li> <li>Maintains compatibility with existing video libraries</li> <li>Enables selective frame loading for large datasets</li> <li>Prevents metadata files from becoming unwieldy</li> </ul> <p>Why standardization matters?</p> <p>Without OWAMcap, the desktop automation field risks repeating robotics' mistakes: fragmented datasets, wasted conversion efforts, and limited foundation model potential. By establishing this standard early, we enable the community to focus on advancing capabilities rather than solving compatibility problems.</p> <p>The Bottom Line</p> <p>OWAMcap transforms desktop interaction data from isolated, proprietary collections into a unified resource for building the next generation of foundation models. It's not just a file format\u2014it's the infrastructure for collaborative progress in desktop automation.</p>"},{"location":"data/how_to_explorer_and_edit/","title":"Exploring &amp; Editing OWAMcap","text":""},{"location":"data/how_to_explorer_and_edit/#sample-datasets","title":"Sample Datasets","text":"<p>Below are sample datasets you can download and explore:</p> <ul> <li><code>example.mcap</code> [Download]</li> <li><code>example.mkv</code> [Download]</li> </ul> Click here to see <code>example.mkv</code>! <p> </p>"},{"location":"data/how_to_explorer_and_edit/#how-to-explore-the-dataset","title":"How to Explore the Dataset","text":"<p>There are multiple ways to explore OWAMcap files. Here are three methods:</p>"},{"location":"data/how_to_explorer_and_edit/#1-owa-dataset-visualizer","title":"1. OWA Dataset Visualizer","text":"<p>Click <code>Choose File</code> at <code>Upload Files</code>. Note that uploading file is inappropriate for large file. To visualize large file, self-host dataset visualizer by your own. Learn more...</p>"},{"location":"data/how_to_explorer_and_edit/#2-using-the-owl-command-line-tool","title":"2. Using the <code>owl</code> Command Line Tool","text":"<p>The <code>owl</code> (Open World agents cLi) tool provides a convenient way to inspect MCAP files.</p>"},{"location":"data/how_to_explorer_and_edit/#getting-a-summary","title":"Getting a Summary","text":"<p>View a summary of the MCAP file:</p> <pre><code>$ owl mcap info example.mcap\nlibrary:   mcap-owa-support 0.1.0; mcap 1.2.2\nprofile:   owa\nmessages:  518\nduration:  6.8558623s\nstart:     2025-03-21T17:06:30.7029335+09:00 (1742544390.702933500)\nend:       2025-03-21T17:06:37.5587958+09:00 (1742544397.558795800)\ncompression:\n        zstd: [1/1 chunks] [48.19 KiB/9.42 KiB (80.44%)] [1.37 KiB/sec]\nchannels:\n        (1) window            7 msgs (1.02 Hz)    : owa.env.desktop.msg.WindowInfo [jsonschema]\n        (2) keyboard/state    7 msgs (1.02 Hz)    : owa.env.desktop.msg.KeyboardState [jsonschema]\n        (3) mouse/state       7 msgs (1.02 Hz)    : owa.env.desktop.msg.MouseState [jsonschema]\n        (4) mouse           115 msgs (16.77 Hz)   : owa.env.desktop.msg.MouseEvent [jsonschema]\n        (5) screen          362 msgs (52.80 Hz)   : owa.env.gst.msg.ScreenEmitted [jsonschema]\n        (6) keyboard         20 msgs (2.92 Hz)    : owa.env.desktop.msg.KeyboardEvent [jsonschema]\nchannels: 6\nattachments: 0\nmetadata: 0\n</code></pre>"},{"location":"data/how_to_explorer_and_edit/#examining-message-content","title":"Examining Message Content","text":"<p>Inspect detailed messages (note that the output below is a created example):</p> <pre><code>$ owl mcap cat example.mcap --n 8 --no-pretty\nTopic: window, Timestamp: 1741628814049712700, Message: {'title': 'ZType \u2013 Typing Game - Type to Shoot - Chromium', 'rect': [389, 10, 955, 1022], 'hWnd': 7540094}\nTopic: keyboard/state, Timestamp: 1741628814049712700, Message: {'buttons': []}\nTopic: mouse/state, Timestamp: 1742544390703436600, Message: {'x': 1594, 'y': 1112, 'buttons': []}\nTopic: mouse, Timestamp: 1742544390707441200, Message: {'event_type': 'move', 'x': 1597, 'y': 1112}\nTopic: screen, Timestamp: 1741628814057575300, Message: {'path': 'example.mkv', 'pts': 14866666666, 'utc_ns': 1741628814056571100}\nTopic: screen, Timestamp: 1741628814073392700, Message: {'path': 'example.mkv', 'pts': 14883333333, 'utc_ns': 1741628814072476900}\nTopic: keyboard, Timestamp: 1741628815015522100, Message: {'event_type': 'release', 'vk': 162}\n</code></pre>"},{"location":"data/how_to_explorer_and_edit/#3-using-owamcapreader-in-python","title":"3. Using <code>OWAMcapReader</code> in Python","text":"<p>You can programmatically access the MCAP data using the Python API:</p> <pre><code>from mcap_owa.highlevel import OWAMcapReader\n\ndef main():\n    with OWAMcapReader(\"tmp/example.mcap\") as reader:\n        # Print available topics and time range\n        print(reader.topics)\n        print(reader.start_time, reader.end_time)\n\n        # Iterate through all messages\n        for topic, timestamp, msg in reader.iter_decoded_messages():\n            print(f\"Topic: {topic}, Timestamp: {timestamp}, Message: {msg}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"data/how_to_explorer_and_edit/#4-using-a-media-player-eg-vlc","title":"4. Using a Media Player (e.g., VLC)","text":"<p>For visual exploration of the data:</p> <ol> <li> <p>Convert MCAP to SRT subtitle format:    <pre><code># This command converts abcd.mcap into abcd.srt\nowl mcap convert abcd.mcap\n</code></pre></p> </li> <li> <p>Open the .mkv file with a media player that supports subtitles. We recommend VLC media player. You may also check <code>example.srt</code> [Download]</p> </li> </ol>"},{"location":"data/how_to_explorer_and_edit/#how-to-edit-owamcap-files","title":"How to Edit OWAMcap Files","text":"<p>You can create and modify OWAMcap files using the Python API. The example below demonstrates writing and reading messages:</p> <pre><code>import tempfile\n\nfrom mcap_owa.highlevel import OWAMcapReader, OWAMcapWriter\nfrom owa.core.message import OWAMessage\nfrom owa.env.desktop.msg import KeyboardEvent\n\n\nclass String(OWAMessage):\n    _type = \"std_msgs/String\"\n    data: str\n\n\ndef main():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        file_path = tmpdir + \"/output.mcap\"\n\n        # Writing messages to an OWAMcap file\n        with OWAMcapWriter(file_path) as writer:\n            for i in range(0, 10):\n                publish_time = i\n                if i % 2 == 0:\n                    topic = \"/chatter\"\n                    event = String(data=\"string message\")\n                else:\n                    topic = \"/keyboard\"\n                    event = KeyboardEvent(event_type=\"press\", vk=1)\n                writer.write_message(topic, event, publish_time=publish_time)\n\n        # Reading messages from an OWAMcap file\n        with OWAMcapReader(file_path) as reader:\n            for topic, timestamp, msg in reader.iter_decoded_messages():\n                print(f\"Topic: {topic}, Timestamp: {timestamp}, Message: {msg}\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Example output:</p> <pre><code>Topic: /chatter, Timestamp: 1741767097157638598, Message: {'data': 'string message'}\nTopic: /keyboard, Timestamp: 1741767097157965764, Message: {'event_type': 'press', 'vk': 1}\nTopic: /chatter, Timestamp: 1741767097157997762, Message: {'data': 'string message'}\nTopic: /keyboard, Timestamp: 1741767097158019602, Message: {'event_type': 'press', 'vk': 1}\nTopic: /chatter, Timestamp: 1741767097158036925, Message: {'data': 'string message'}\nTopic: /keyboard, Timestamp: 1741767097158051239, Message: {'event_type': 'press', 'vk': 1}\nTopic: /chatter, Timestamp: 1741767097158065463, Message: {'data': 'string message'}\nTopic: /keyboard, Timestamp: 1741767097158089318, Message: {'event_type': 'press', 'vk': 1}\nTopic: /chatter, Timestamp: 1741767097158113250, Message: {'data': 'string message'}\nTopic: /keyboard, Timestamp: 1741767097158129738, Message: {'event_type': 'press', 'vk': 1}\n</code></pre>"},{"location":"data/ocap/","title":"ocap","text":"<p>High-performance desktop recorder for Windows. Captures screen, audio, keyboard, mouse, and window events.</p>"},{"location":"data/ocap/#what-is-ocap","title":"What is ocap?","text":"<p>ocap (Omnimodal CAPture) captures all essential desktop signals in synchronized format. Records screen video, audio, keyboard/mouse input, and window events. Built for the open-world-agents project but works for any desktop recording needs.</p> <p>TL;DR: Complete, high-performance desktop recording tool for Windows. Captures everything in one command.</p>"},{"location":"data/ocap/#key-features","title":"Key Features","text":"<ul> <li>Complete desktop recording: Video, audio, keyboard/mouse events, window events</li> <li>High performance: Hardware-accelerated with Windows APIs and GStreamer</li> <li>Efficient encoding: H265/HEVC for high quality and small file size</li> <li>Simple operation: <code>ocap FILE_LOCATION</code> (stop with Ctrl+C)</li> <li>Clean architecture: Core logic in single 250-line Python file</li> <li>Modern formats: MKV with embedded timestamps, MCAP format for events</li> </ul>"},{"location":"data/ocap/#system-requirements","title":"System Requirements","text":"<p>Based on OBS Studio recommended specs + NVIDIA GPU requirements:</p> Component Specification OS Windows 11 (64-bit) Processor Intel i7 8700K / AMD Ryzen 1600X Memory 8 GB RAM Graphics NVIDIA GeForce 10 Series or newer \u26a0\ufe0f DirectX Version 11 Storage 600 MB + ~100MB per minute recording <p>\u26a0\ufe0f NVIDIA GPU Required: Currently only supports NVIDIA GPUs for hardware acceleration. AMD/Intel GPU support possible through GStreamer framework - contributions welcome!</p>"},{"location":"data/ocap/#installation-usage","title":"Installation &amp; Usage","text":""},{"location":"data/ocap/#option-1-download-release","title":"Option 1: Download Release","text":"<ol> <li>Download <code>ocap.zip</code> from releases</li> <li>Unzip and run:<ul> <li>Double-click <code>run.bat</code> (opens terminal with virtual environment)</li> <li>Or in CLI: <code>run.bat --help</code></li> </ul> </li> </ol>"},{"location":"data/ocap/#option-2-conda-install","title":"Option 2: Conda Install","text":"<pre><code>$ conda install ocap\n</code></pre>"},{"location":"data/ocap/#basic-usage","title":"Basic Usage","text":"<pre><code># Start recording (stop with Ctrl+C)\n$ ocap my-recording\n\n# Show all options\n$ ocap --help\n\n# Advanced options\n$ ocap FILENAME --window-name \"App\"   # Record specific window\n$ ocap FILENAME --monitor-idx 1       # Record specific monitor\n$ ocap FILENAME --fps 60              # Set framerate\n$ ocap FILENAME --no-record-audio     # Disable audio\n</code></pre>"},{"location":"data/ocap/#output-files","title":"Output Files","text":"<ul> <li><code>.mcap</code> \u2014 Event log (keyboard, mouse, windows)</li> <li><code>.mkv</code>  \u2014 Video/audio with embedded timestamps</li> </ul> <p>Your recording files will be ready immediately!</p>"},{"location":"data/ocap/#feature-comparison","title":"Feature Comparison","text":"Feature ocap OBS wcap pillow/mss Advanced data formats (MCAP/MKV) \u2705 Yes \u274c No \u274c No \u274c No Timestamp aligned logging \u2705 Yes \u274c No \u274c No \u274c No Customizable event definition &amp; Listener \u2705 Yes \u274c No \u274c No \u274c No Single python file \u2705 Yes \u274c No \u274c No \u274c No Audio + Window + Keyboard + Mouse \u2705 Yes \u26a0\ufe0f Partial \u274c No \u274c No Hardware-accelerated encoder \u2705 Yes \u2705 Yes \u2705 Yes \u274c No Supports latest Windows APIs \u2705 Yes \u2705 Yes \u2705 Yes \u274c No (legacy APIs only) Optional mouse cursor capture \u2705 Yes \u2705 Yes \u2705 Yes \u274c No"},{"location":"data/ocap/#technical-architecture","title":"Technical Architecture","text":"<p>Built on GStreamer with clean, maintainable design:</p> <pre><code>flowchart TD\n    %% Input Sources\n    A[owa.env.desktop] --&gt; B[Keyboard Events]\n    A --&gt; C[Mouse Events] \n    A --&gt; D[Window Events]\n    E[owa.env.gst] --&gt; F[Screen Capture]\n    E --&gt; G[Audio Capture]\n\n    %% Core Processing\n    B --&gt; H[Event Queue]\n    C --&gt; H\n    D --&gt; H\n    F --&gt; H\n    F --&gt; I[Video/Audio Pipeline]\n    G --&gt; I\n\n    %% Outputs\n    H --&gt; J[MCAP Writer]\n    I --&gt; K[MKV Pipeline]\n\n    %% Files\n    J --&gt; L[\ud83d\udcc4 events.mcap]\n    K --&gt; M[\ud83c\udfa5 video.mkv]\n\n    style A fill:#e1f5fe\n    style E fill:#e1f5fe\n    style H fill:#fff3e0\n    style L fill:#e8f5e8\n    style M fill:#e8f5e8</code></pre> <ul> <li>Easy to verify: Extensive OWA's Env design enables customizable <code>record.py</code></li> <li>Native performance: Direct Windows API integration (DXGI/WGC, WASAPI)</li> </ul>"},{"location":"data/ocap/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Record terminates right after start? Re-run same command multiple times. It's because of crash in GStreamer, where the cause is unknown.</li> <li>Audio not recording? In default setting, only audio from target process is recorded. You can turn off by manually editing gstreamer pipeline.</li> <li>Large file sizes? Easiest way to reduce file size is adjusting <code>gop-size</code> parameter in <code>nvd3d11h265enc</code> element. Check pipeline.py</li> <li>Performance tips: Close unnecessary applications before recording, use SSD storage for better write performance, record to different drive than your OS drive</li> </ul>"},{"location":"data/ocap/#faq","title":"FAQ","text":"<ul> <li>How much disk space do recordings use? ~100MB per minute for 1080p H265 recording.</li> <li>Can I customize recorded events? Yes. Enable/disable audio, keyboard, mouse, and window events individually. Since record.py is just a 250-line single python script, you may customize it easily.</li> <li>Will ocap slow down my computer? Minimal impact with hardware acceleration. Designed for low overhead.</li> <li>What formats are supported? MKV with H265/HEVC encoding for video and MCAP format for events for efficient storage and querying is supported, but you may customize it easily. (e.g. saving <code>jsonl</code> instead of <code>mcap</code> file takes minimal effort by editing record.py)</li> </ul>"},{"location":"data/ocap/#when-to-use-ocap","title":"When to Use ocap","text":"<ul> <li>Agent training: Capture all inputs and outputs for AI training</li> <li>Workflow documentation: Record exact steps with precise timing</li> <li>Performance testing: Low-overhead recording during intensive tasks</li> <li>Complete screen recording: When you need more than just video</li> </ul>"},{"location":"data/viewer/","title":"Viewer for OWAMcap","text":"<p>We provide a web-based viewer for users to easily visualize and check OWAMcap datasets.</p>"},{"location":"data/viewer/#public-hosted","title":"Public hosted","text":"<p>We offer a public hosted viewer at https://huggingface.co/spaces/open-world-agents/visualize_dataset.</p> <p>You can provide a huggingface repo id, or you can also upload your own OWAMcap dataset file via the viewer. Note that this public hosted viewer has a 100MB upload file size limit. If you need to upload larger files, you may self-host the viewer.</p>"},{"location":"data/viewer/#self-hosted","title":"Self hosted","text":"<ol> <li>Go to <code>projects/owa-mcap-viewer</code> directory.</li> <li>Setup <code>EXPORT_PATH</code> environment variable. You may setup <code>.env</code> or use <code>export</code> command.     <pre><code>export EXPORT_PATH=(path-to-your-folder-containing-mcap-and-mkvs)\n</code></pre></li> <li>Run <code>vuv install</code> for installing dependencies.</li> <li>Run the server with <code>uvicorn owa_viewer:app --host 0.0.0.0 --port 7860 --reload</code></li> <li>Access <code>http://localhost:7860</code> in your browser.</li> </ol>"},{"location":"env/","title":"Introducing OWA's Env","text":"<p>Open World Agents (OWA) introduces Env, a groundbreaking modular agent system designed for dynamic, real-time environments. Say goodbye to rigid frameworks with fixed interfaces\u2014Env's flexible architecture lets you activate and customize components on the fly.</p>"},{"location":"env/#why-choose-owas-env","title":"Why Choose OWA's Env?","text":"<p>Traditional environmental interfaces like gymnasium.Env fall short when it comes to building real-time, real-world agents. They rely on synchronous steps (<code>env.step()</code>, <code>env.reset()</code>), which assume your agent has infinite time to process actions. That's not realistic for agents that need to react instantly in dynamic environments.</p> <p>Env changes the game with an event-driven, asynchronous design that mirrors real-world interactions. Here's what sets it apart:</p> <ul> <li> <p>Asynchronous Event Processing: Leverage <code>Callables</code>, <code>Listeners</code>, and <code>Runnables</code> for real-time interaction. No more waiting for <code>env.step()</code>\u2014the world doesn't stop, and neither should your agent.</p> </li> <li> <p>Dynamic EnvPlugin Activation: Seamlessly register and activate <code>EnvPlugins</code> at runtime to extend functionality, powered by registry pattern. Learn how to create custom plugins.</p> </li> <li> <p>Extensible, Open-Source Design: Built for the community, by the community. Easily add custom plugins and extend the Env's functionality to suit your needs.</p> </li> </ul>"},{"location":"env/#the-future-is-real-time","title":"The Future is Real-Time","text":"<p>Time waits for no one\u2014and neither do real-world agents. As we advance towards more responsive AI, agents must be capable of instantaneous reactions, just like humans. Env's architecture enables:</p> <ul> <li> <p>True Concurrent Processing: Handle multiple events simultaneously without bottlenecks.</p> </li> <li> <p>Measured Reaction Times: Agents operate within realistic timeframes, ensuring timely responses in dynamic settings.</p> </li> </ul> <p>We prioritize minimizing latency within the framework, aiming for agent reaction times that match or surpass human capabilities. Throughout our codebase, we ensure latency doesn't exceed 30ms. Check out how we achieve this in our Screen Listeners, and Test Screen Listener.</p>"},{"location":"env/#get-started-today","title":"Get Started Today","text":"<p>Don't let outdated frameworks hold you back. Embrace the future with OWA's Env and build agents that are ready for the real world.</p> <p>Learn more about OWA's Env Design.</p>"},{"location":"env/custom_plugins/","title":"How to write your own EnvPlugin","text":"<p>You may write &amp; contribute your own EnvPlugin.</p> <ol> <li>Copy &amp; Paste owa-env-example directory. This directory contains following:     <pre><code>owa-env-example\n\u251c\u2500\u2500 owa/env/example\n\u2502   \u251c\u2500\u2500 example_callable.py\n\u2502   \u251c\u2500\u2500 example_listener.py\n\u2502   \u251c\u2500\u2500 example_runnable.py\n\u2502   \u2514\u2500\u2500 __init__.py\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 tests\n\u2502   \u2514\u2500\u2500 test_print.py\n\u2514\u2500\u2500 uv.lock\n</code></pre></li> <li>Rename <code>owa-env-example</code> to your own EnvPlugin's name.</li> <li>Write your own code in the specific source folder.<ul> <li>Important: To maintain the namespace package structure, all source files must only be written inside the <code>owa/env/example</code> folder.</li> <li>What NOT to do: Don't place source files in paths between <code>owa</code> and <code>owa/env/example</code> (e.g., <code>owa/some_file.py</code> or <code>owa/env/some_file.py</code>).</li> <li>Correct structure:     <pre><code>owa\n\u2514\u2500\u2500 env\n    \u2514\u2500\u2500 example\n        \u251c\u2500\u2500 your_code.py\n        \u251c\u2500\u2500 your_module.py\n        \u2514\u2500\u2500 __init__.py\n</code></pre></li> </ul> </li> <li>Make sure your repository contains all dependencies. We recommend you to use <code>uv</code> as package manager.</li> <li>Make a PR, following Contributing Guide</li> </ol>"},{"location":"env/guide/","title":"Comprehensive Guide for Env","text":""},{"location":"env/guide/#core-concepts","title":"Core Concepts","text":""},{"location":"env/guide/#three-main-components","title":"Three Main Components:","text":"<p>Open World Agents (OWA)'s Env consists of three primary components that enable interaction with the environment in different ways.</p> <ol> <li> <p>Callable - Functions you actively call to perform actions or get state</p> <ul> <li>These are like traditional function calls; you invoke them when you need to perform an action or retrieve some information from the environment.</li> <li>Implements <code>__call__</code> function</li> <li>Example: <code>CALLABLES[\"clock.time_ns\"]()</code></li> </ul> </li> <li> <p>Listener - Components that respond to events and execute your callbacks</p> <ul> <li>Listeners wait for specific events and execute your callback functions when those events occur.</li> <li>Takes a <code>callback</code> parameter in the <code>configure</code> method</li> <li>Example: <pre><code>listener = LISTENERS[\"keyboard\"]().configure(callback=my_callback)\nwith listener.session:\n    input(\"Type enter to exit.\")\n</code></pre> This example sets up a keyboard listener that invokes <code>my_callback</code> whenever a keyboard event is detected.</li> </ul> </li> <li> <p>Runnable - Background processes that can be started and stopped</p> <ul> <li>Runnables run in the background and can be managed with start and stop operations.</li> <li>Parent class of <code>Listener</code>, the only difference is absence of <code>callback</code> argument in <code>configure</code>.</li> <li>Supports <code>start()</code>, <code>stop()</code>, and <code>join()</code> operations</li> </ul> </li> </ol> <p>What's the difference between Callable and Listener?</p> <p>The key difference between these two is who initiates the call:</p> <ul> <li>In Callable, caller actively executes the Callable.</li> <li>In Listener, callee waits for events and then calls user-provided \"callbacks\".</li> </ul> <p>In other words, Callables are synchronous functions you call directly, while Listeners are asynchronous and react to events.</p> <p>Common environmental interfaces such as gymnasium.Env only provides object/method equivalent to Callable.</p>"},{"location":"env/guide/#registry-system","title":"Registry System","text":"<p>The OWA environment uses a registry system to manage and access the various components.</p> <p>Components are managed through global registries:</p> <ul> <li> <p><code>CALLABLES</code> - Dictionary of callable functions</p> </li> <li> <p><code>LISTENERS</code> - Dictionary of event listeners</p> </li> <li> <p><code>RUNNABLES</code> - Dictionary of background processes  </p> </li> </ul> <p>Modules are activated using: <pre><code>from owa.core.registry import activate_module\nactivate_module(\"module.name\")\n</code></pre> Activating a module registers its components into the global registries for use.</p>"},{"location":"env/guide/#environment-usage-examples","title":"Environment Usage Examples","text":""},{"location":"env/guide/#standard-environment-owaenvstd","title":"Standard Environment (<code>owa.env.std</code>)","text":"<p>Here is an example of how to use the standard environment to interact with clock functionalities.</p> <p><pre><code>import time\nfrom owa.core.registry import CALLABLES, LISTENERS, activate_module\n\n# Initial registry state (empty)\nprint(CALLABLES, LISTENERS)  # {}, {}\n\n# Activate the standard module to register clock functionalities\nactivate_module(\"owa.env.std\")\nprint(CALLABLES, LISTENERS)\n# {'clock.time_ns': &lt;built-in function time_ns&gt;} {'clock/tick': &lt;class 'owa.env.std.clock.ClockTickListener'&gt;}\n\n# Testing the clock/tick listener\ntick = LISTENERS[\"clock/tick\"]().configure(callback=lambda: print(CALLABLES[\"clock.time_ns\"]()), interval=1)\ntick.start()\n\ntime.sleep(2)  # The listener prints the current time in nanoseconds a few times\n\ntick.stop(), tick.join()\n</code></pre> In this example, we activate the standard module, which registers clock functions and listeners. We then set up a tick listener that prints the current time every second.</p> <p>Instead of manual <code>start-stop-join</code> procedure, you may utilize context manager: <code>.session</code>! Following example shows how to abbreviate <code>start-stop-join</code> steps.</p> <pre><code>with tick.session:\n    time.sleep(2)\n</code></pre>"},{"location":"env/guide/#desktop-environment-owaenvdesktop","title":"Desktop Environment (<code>owa.env.desktop</code>)","text":"<p>The desktop environment module provides capabilities for UI interaction and input handling.</p> <p><pre><code>from owa.core.registry import CALLABLES, LISTENERS, activate_module\nfrom owa.env.desktop.msg import KeyboardEvent\n\n# Activate the desktop module to enable UI and input capabilities\nactivate_module(\"owa.env.desktop\")\n\n# Using screen capture and window management features\nprint(f\"{CALLABLES['screen.capture']().shape=}\")  # Example output: (1080, 1920, 3)\nprint(f\"{CALLABLES['window.get_active_window']()=}\")\nprint(f\"{CALLABLES['window.get_window_by_title']('open-world-agents')=}\")\n\n# Simulating a mouse click (left button, double click)\nmouse_click = CALLABLES[\"mouse.click\"]\nmouse_click(\"left\", 2)\n\n\n# Configuring a keyboard listener\ndef on_keyboard_event(keyboard_event: KeyboardEvent):\n    print(f\"Keyboard event: {keyboard_event.event_type=}, {keyboard_event.vk=}\")\n\n\nkeyboard_listener = LISTENERS[\"keyboard\"]().configure(callback=on_keyboard_event)\nwith keyboard_listener.session:\n    input(\"Type enter to exit.\\n\")\n</code></pre> This code demonstrates capturing the screen, retrieving window information, simulating mouse clicks, and listening to keyboard events.</p>"},{"location":"env/guide/#custom-envplugin-example","title":"Custom EnvPlugin Example","text":"<p>Also, you can register your own EnvPlugin which contains custom Callable, Listener, or Runnable. For more information, see Custom EnvPlugin.</p> <p>Creating custom plugins allows you to extend the OWA environment with your own functionalities.</p> <p><pre><code>import time\nfrom owa.core import Listener\nfrom owa.core.registry import CALLABLES, LISTENERS\n\n# Register your custom callables\n@CALLABLES.register(\"my_module.add\")\ndef __call__(a, b):\n    return a + b\n\n# Register a custom listener\n@LISTENERS.register(\"my_module/events\")\nclass EventListener(Listener):\n    def on_configure(self, *args):\n        self.args = args\n\n    def loop(self, *, stop_event, callback):\n        while not stop_event.is_set():\n            callback(\"Event\")\n            time.sleep(1)\n\n# Using the custom module\nresult = CALLABLES[\"my_module.add\"](5, 3)  # Returns 8\n</code></pre> In this example, we define a custom module with an add function and a custom event listener.</p>"},{"location":"env/guide/#architecture-summary","title":"Architecture Summary","text":"<p>The diagram below summarizes the architecture of the OWA environment and how components are registered and used.</p> <pre><code>graph LR;\n    R[Registry] --&gt;|Registers| SM[\"Standard Module(owa.env.std)\"]\n    R --&gt;|Registers| DM[\"Desktop Module(owa.env.desktop)\"]\n    SM --&gt;|Provides| C1[clock.time_ns]\n    SM --&gt;|Provides| L1[clock/tick Listener]\n    DM --&gt;|Provides| C2[screen.capture]\n    DM --&gt;|Provides| C3[window.get_active_window]\n    DM --&gt;|Provides| L2[keyboard Listener]\n    User --&gt;|Activates| AM[activate_module]\n    AM --&gt; R</code></pre>"},{"location":"env/guide/#additional-resources","title":"Additional Resources","text":"<ul> <li>For standard module details: owa-env-std</li> <li>For desktop features: owa-env-desktop</li> <li>For multimedia support: owa-env-gst</li> <li>For custom EnvPlugin development: custom_plugins.md</li> </ul>"},{"location":"env/plugins/desktop_env/","title":"Desktop Environment","text":"<p>The Desktop Environment module (owa.env.desktop) extends Open World Agents by providing functionalities that interact with the operating system's desktop. It focuses on user interface interactions and input simulation.</p>"},{"location":"env/plugins/desktop_env/#features","title":"Features","text":"<ul> <li>Screen Capture: Capture the current screen using CALLABLES[\"screen.capture\"].</li> <li>Window Management: Retrieve information about active windows and search for windows by title using functions like CALLABLES[\"window.get_active_window\"] and CALLABLES[\"window.get_window_by_title\"].</li> <li>Input Simulation: Simulate mouse actions (e.g., CALLABLES[\"mouse.click\"]) and set up keyboard listeners to handle input events.</li> </ul>"},{"location":"env/plugins/desktop_env/#usage","title":"Usage","text":"<p>To activate the Desktop Environment module, include the following in your code:</p> <pre><code>activate_module(\"owa.env.desktop\")\n</code></pre> <p>After activation, you can access desktop functionalities via the global registries. For example:</p> <pre><code>print(CALLABLES[\"screen.capture\"]().shape)  # Capture and display screen dimensions\nprint(CALLABLES[\"window.get_active_window\"]())  # Retrieve the active window\n</code></pre> <p>This module is essential for applications that require integration with desktop UI elements and user input simulation.</p>"},{"location":"env/plugins/desktop_env/#implementation-details","title":"Implementation Details","text":"<p>To see detailed implementation, skim over owa-env-desktop. API documentation is currently being developed.</p>"},{"location":"env/plugins/desktop_env/#available-functions","title":"Available Functions","text":""},{"location":"env/plugins/desktop_env/#mouse-functions","title":"Mouse Functions","text":"<ul> <li><code>mouse.click</code> - Simulate a mouse click</li> <li><code>mouse.move</code> - Move the mouse cursor to specified coordinates</li> <li><code>mouse.position</code> - Get the current mouse position</li> <li><code>mouse.press</code> - Simulate pressing a mouse button</li> <li><code>mouse.release</code> - Simulate releasing a mouse button</li> <li><code>mouse.scroll</code> - Simulate mouse wheel scrolling</li> </ul>"},{"location":"env/plugins/desktop_env/#keyboard-functions","title":"Keyboard Functions","text":"<ul> <li><code>keyboard.press</code> - Simulate pressing a keyboard key</li> <li><code>keyboard.release</code> - Simulate releasing a keyboard key</li> <li><code>keyboard.type</code> - Type a string of characters</li> <li><code>keyboard.press_repeat</code> - Simulate repeat-press when pressing key long time</li> </ul>"},{"location":"env/plugins/desktop_env/#screen-functions","title":"Screen Functions","text":"<ul> <li><code>screen.capture</code> - Capture the current screen (Note: This module utilizes <code>bettercam</code>. For better performance and extensibility, use <code>owa-env-gst</code>'s functions instead)</li> </ul>"},{"location":"env/plugins/desktop_env/#window-functions","title":"Window Functions","text":"<ul> <li><code>window.get_active_window</code> - Get the currently active window</li> <li><code>window.get_window_by_title</code> - Find a window by its title</li> <li><code>window.when_active</code> - Run a function when a specific window becomes active</li> </ul>"},{"location":"env/plugins/desktop_env/#available-listeners","title":"Available Listeners","text":"<ul> <li><code>keyboard</code> - Listen for keyboard events</li> <li><code>mouse</code> - Listen for mouse events</li> </ul>"},{"location":"env/plugins/desktop_env/#misc","title":"Misc","text":""},{"location":"env/plugins/desktop_env/#library-selection-rationale","title":"Library Selection Rationale","text":"<p>This module utilizes <code>pynput</code> for input simulation after evaluating several alternatives:</p> <ul> <li> <p>Why not PyAutoGUI? Though widely used, PyAutoGUI uses deprecated Windows APIs (<code>keybd_event/mouse_event</code>) rather than the modern <code>SendInput</code> method. These older APIs fail in DirectX applications and games. Additionally, PyAutoGUI has seen limited maintenance (last significant update was over 2 years ago).</p> </li> <li> <p>Alternative Solutions: Libraries like pydirectinput and pydirectinput_rgx address the Windows API issue by using <code>SendInput</code>, but they lack input capturing capabilities which are essential for our use case.</p> </li> <li> <p>Other Options: We also evaluated keyboard and mouse libraries but found them inadequately maintained with several unresolved bugs that could impact reliability.</p> </li> </ul>"},{"location":"env/plugins/desktop_env/#input-auto-repeat-functionality","title":"Input Auto-Repeat Functionality","text":"<p>For simulating key auto-repeat behavior, use the dedicated function:</p> <pre><code>CALLABLES[\"keyboard.press_repeat\"](key, press_time: float, initial_delay: float = 0.5, repeat_delay: float = 0.033)\n</code></pre> <p>This function handles the complexity of simulating hardware auto-repeat, with configurable initial delay before repeating starts and the interval between repeated keypresses.</p>"},{"location":"env/plugins/gstreamer_env/","title":"Gstreamer Environment","text":"<p>To see detailed implementation, skim over owa_env_gst. API Docs is being written WIP.</p>"},{"location":"env/plugins/gstreamer_env/#examples","title":"Examples","text":"<ul> <li> <p>example of <code>screen</code> listener     <pre><code>from owa.core.registry import LISTENERS, activate_module\nimport cv2\nimport numpy as np\n\n# Activate the GStreamer module\nactivate_module(\"owa.env.gst\")\n\n# Define a callback to process frames\ndef process_frame(frame):\n    # Display the frame\n    cv2.imshow(\"Screen Capture\", frame.frame_arr)\n    cv2.waitKey(1)\n\n# Create and configure the listener\nscreen = LISTENERS[\"screen\"]().configure(\n    callback=process_frame,\n    fps=30,\n    show_cursor=True\n)\n\n# Run the screen capture\nwith screen.session:\n    input(\"Press Enter to stop\")\n</code></pre></p> <p>For performance metrics: <pre><code>def process_with_metrics(frame, metrics):\n    print(f\"FPS: {metrics.fps:.2f}, Latency: {metrics.latency*1000:.2f} ms\")\n    cv2.imshow(\"Screen\", frame.frame_arr)\n    cv2.waitKey(1)\n\nscreen.configure(callback=process_with_metrics)\n</code></pre></p> </li> <li> <p>example of <code>screen_capture</code> runnable     <pre><code>from owa.core.registry import RUNNABLES, activate_module\n\nactivate_module(\"owa.env.gst\")\nscreen_capture = RUNNABLES[\"screen_capture\"]().configure(fps=60)\n\nwith screen_capture.session:\n    for _ in range(10):\n        frame = screen_capture.grab()\n        print(f\"Shape: {frame.frame_arr.shape}\")\n</code></pre></p> </li> </ul>"},{"location":"env/plugins/gstreamer_env/#known-issues","title":"Known Issues","text":"<ul> <li>Currently, we only supports Windows OS. Other OS support is in TODO-list, but it's priority is not high.</li> <li> <p>Currently, we only supports device with NVIDIA GPU. This is also in TODO-list, it's priority is higher than multi-OS support.</p> </li> <li> <p>When capturing some screen with <code>WGC</code>(Windows Graphics Capture API, it's being activate when you specify window handle), and with some desktop(not all), below issues are observed.</p> <ul> <li>maximum FPS can't exceed maximum Hz of physical monitor.</li> <li>When capturing <code>Windows Terminal</code> and <code>Discord</code>, the following case was reported. I also guess this phenomena is because of usage of <code>WGC</code>.<ul> <li>When there's no change in window, FPS drops to 1-5 frame.</li> <li>When there's change(e.g. mouse movement) in window, FPS straightly recovers to 60+.</li> </ul> </li> </ul> </li> </ul>"},{"location":"env/plugins/std/","title":"Standard Environment Plugin","text":"<p>The Standard Environment plugin (<code>owa.env.std</code>) is a core component of the Open World Agents framework. It provides essential functionalities related to time management and clock operations, which are fundamental for various time-based tasks and event scheduling within the system.</p>"},{"location":"env/plugins/std/#features","title":"Features","text":"<ul> <li>Time Functions: The plugin registers functions like <code>clock.time_ns</code> that return the current time in nanoseconds.</li> <li>Tick Listener: It includes a <code>clock/tick</code> listener that can be configured to execute callbacks at specified intervals.</li> </ul>"},{"location":"env/plugins/std/#usage","title":"Usage","text":"<p>To activate the Standard Environment plugin, use the following command in your code:</p> <pre><code>from owa.core.registry import activate_module\n\nactivate_module(\"owa.env.std\")\n</code></pre> <p>Once activated, you can access the registered functions and listeners via the global <code>CALLABLES</code> and <code>LISTENERS</code> registries. For example:</p> <pre><code>from owa.core.registry import CALLABLES, LISTENERS\n\n# Get the current time in nanoseconds\ncurrent_time_ns = CALLABLES[\"clock.time_ns\"]()\nprint(f\"Current time (ns): {current_time_ns}\")\n\n# Configure and start a tick listener\ndef on_tick():\n    print(f\"Tick at {CALLABLES['clock.time_ns']()}\")\n\ntick_listener = LISTENERS[\"clock/tick\"]()\ntick_listener.configure(callback=on_tick, interval=1)  # Tick every second\ntick_listener.start()\n\n# Run for a few seconds to see the tick listener in action\nimport time\ntime.sleep(5)\n\n# Stop the tick listener\ntick_listener.stop()\ntick_listener.join()\n</code></pre>"},{"location":"env/plugins/std/#components","title":"Components","text":""},{"location":"env/plugins/std/#time-functions","title":"Time Functions","text":"<ul> <li><code>clock.time_ns</code>: Returns the current time in nanoseconds. This function is registered in the <code>CALLABLES</code> registry.</li> </ul>"},{"location":"env/plugins/std/#tick-listener","title":"Tick Listener","text":"<ul> <li><code>clock/tick</code>: A listener that triggers a callback at specified intervals. This listener is registered in the <code>LISTENERS</code> registry and can be configured with an interval in seconds.</li> </ul>"},{"location":"env/plugins/std/#example","title":"Example","text":"<p>Here is a complete example demonstrating how to use the Standard Environment plugin:</p> <pre><code>from owa.core.registry import CALLABLES, LISTENERS, activate_module\n\n# Activate the Standard Environment plugin\nactivate_module(\"owa.env.std\")\n\n# Print the current time in nanoseconds\nprint(CALLABLES[\"clock.time_ns\"]())\n\n# Define a callback function for the tick listener\ndef tick_callback():\n    print(f\"Tick at {CALLABLES['clock.time_ns']()}\")\n\n# Configure and start the tick listener\ntick_listener = LISTENERS[\"clock/tick\"]().configure(callback=tick_callback, interval=1)\ntick_listener.start()\n\n# Let the listener run for 5 seconds\nimport time\ntime.sleep(5)\n\n# Stop the tick listener\ntick_listener.stop()\ntick_listener.join()\n</code></pre> <p>This example demonstrates how to activate the plugin, retrieve the current time, and set up a tick listener that prints the current time every second.</p> <p>The Standard Environment plugin is a fundamental part of the Open World Agents framework, providing essential time-based functionalities that can be leveraged by other modules and applications.</p>"}]}