# OWA Training Image - ML Training Environment
# Multi-stage build to exclude cuda from final image

# Build arguments
ARG BASE_IMAGE=nvidia/cuda:12.6.3-devel-ubuntu24.04
ARG CONDA_INSTALL_PATH=/opt/conda

# =============================================================================
# Stage 1: Runtime Environment
# =============================================================================
FROM ${BASE_IMAGE} AS ml-base

# Build arguments
ARG MINIFORGE_VERSION=latest
ARG CONDA_INSTALL_PATH=/opt/conda

# Environment setup
ENV DEBIAN_FRONTEND=noninteractive 
ENV USER=root UID=0 GID=0 HOME=/root

# Install system dependencies first (rarely changes)
RUN --mount=type=cache,target=/var/cache/apt,sharing=locked \
    --mount=type=cache,target=/var/lib/apt,sharing=locked \
    apt-get update && apt-get install -y --no-install-recommends \
    git curl wget ca-certificates build-essential

# Install miniforge
COPY docker/setup_miniforge.sh /tmp/setup_miniforge.sh
RUN --mount=type=cache,target=/root/.cache,sharing=locked \
    chmod +x /tmp/setup_miniforge.sh && \
    /tmp/setup_miniforge.sh ${MINIFORGE_VERSION} ${CONDA_INSTALL_PATH} && \
    rm -f /tmp/setup_miniforge.sh

ENV VUV_ALLOW_BASE=true PATH="${CONDA_INSTALL_PATH}/bin:${PATH}"

# Default configuration
SHELL ["/bin/bash", "-c"]
CMD ["/bin/bash"]

# required for opencv-python to work, https://stackoverflow.com/a/68666500, https://askubuntu.com/questions/1060903/importerror-libgthread-2-0-so-0-cannot-open-shared-object-file-no-such-file-o
RUN --mount=type=cache,target=/var/cache/apt,sharing=locked \
    --mount=type=cache,target=/var/lib/apt,sharing=locked \
    apt-get update && apt-get install -y --no-install-recommends \
    libgl1 libglib2.0-0

# Project setup arguments
ARG PROJECT_DIR=/workspace
ARG CONDA_INSTALL_PATH=/opt/conda

# Set working directory
WORKDIR ${PROJECT_DIR}

# Copy and run project setup script
COPY docker/setup_runtime.sh /tmp/setup_runtime.sh
RUN --mount=type=cache,target=${HOME}/.cache/uv,sharing=locked \
    --mount=type=cache,target=${HOME}/.cache/pip,sharing=locked \
    --mount=type=cache,target=${CONDA_INSTALL_PATH}/pkgs,sharing=locked \
    chmod +x /tmp/setup_runtime.sh && \
    /tmp/setup_runtime.sh ${PROJECT_DIR} && \
    rm /tmp/setup_runtime.sh

# Install ML packages and projects/agent package
RUN --mount=type=cache,target=${HOME}/.cache/uv,sharing=locked \
    --mount=type=cache,target=${HOME}/.cache/pip,sharing=locked \
    --mount=type=bind,source=open-world-agents/projects/owa-data,target=/workspace/open-world-agents/projects/owa-data \
    --mount=type=bind,source=projects/agent,target=/workspace/projects/agent \
    . activate owa && \
    vuv pip install torch torchvision tqdm einops && \
    vuv pip install /workspace/projects/agent && \
    vuv pip install trl deepspeed

# =============================================================================
# Stage 2: Flash Attention Builder (with CUDA build tools)
# =============================================================================
FROM nvidia/cuda:12.6.3-devel-ubuntu24.04 AS flash-attn-builder

ARG CONDA_INSTALL_PATH

# Environment configuration
ENV DEBIAN_FRONTEND=noninteractive \
    USER=root UID=0 GID=0 HOME=/root \
    VUV_ALLOW_BASE=true PATH="${CONDA_INSTALL_PATH}/bin:${PATH}"

# Set shell for RUN commands
SHELL ["/bin/bash", "-c"]

# Copy conda environment from ml-base stage
COPY --from=ml-base ${CONDA_INSTALL_PATH} ${CONDA_INSTALL_PATH}

# Install build dependencies and compile flash-attn
RUN --mount=type=cache,target=${HOME}/.cache/uv,sharing=locked \
    --mount=type=cache,target=${HOME}/.cache/pip,sharing=locked \
    . activate owa && \
    vuv pip install ninja packaging && \
    pip wheel flash-attn --no-build-isolation -w /tmp/wheels && \
    vuv pip install /tmp/wheels/*.whl

# =============================================================================
# Stage 3: Final Training Image
# =============================================================================
FROM ml-base AS final

ARG CONDA_INSTALL_PATH

# Following command creates +10GB layer, so must not be used.
# COPY --from=flash-attn-builder ${CONDA_INSTALL_PATH}/envs/owa/lib/python3.11/site-packages ${CONDA_INSTALL_PATH}/envs/owa/lib/python3.11/site-packages

# Copy compiled flash-attn from builder stage. Following list can be acquired by "uninstalling" flash-attn, or by `pip show -f flash-attn`
# COPY --from=flash-attn-builder ${CONDA_INSTALL_PATH}/envs/owa/lib/python3.11/site-packages/flash_attn ${CONDA_INSTALL_PATH}/envs/owa/lib/python3.11/site-packages/flash_attn
# COPY --from=flash-attn-builder ${CONDA_INSTALL_PATH}/envs/owa/lib/python3.11/site-packages/flash_attn-2.8.1.dist-info ${CONDA_INSTALL_PATH}/envs/owa/lib/python3.11/site-packages/flash_attn-2.8.1.dist-info
# COPY --from=flash-attn-builder ${CONDA_INSTALL_PATH}/envs/owa/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so ${CONDA_INSTALL_PATH}/envs/owa/lib/python3.11/site-packages/
# COPY --from=flash-attn-builder ${CONDA_INSTALL_PATH}/envs/owa/lib/python3.11/site-packages/hopper ${CONDA_INSTALL_PATH}/envs/owa/lib/python3.11/site-packages/hopper

# Copy and install wheels, much graceful solution than aboves
RUN --mount=type=cache,target=${HOME}/.cache/uv,sharing=locked \
    --mount=type=bind,from=flash-attn-builder,source=/tmp/wheels,target=/tmp/wheels \
    . activate owa && vuv pip install /tmp/wheels/flash_attn-*.whl

# Set working directory
WORKDIR /workspace

# Default command
CMD ["/bin/bash"]
